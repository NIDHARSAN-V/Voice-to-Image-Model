{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in .\\.venv\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: sounddevice in .\\.venv\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: numpy in .\\.venv\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in .\\.venv\\lib\\site-packages (from flask) (3.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in .\\.venv\\lib\\site-packages (from flask) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in .\\.venv\\lib\\site-packages (from flask) (2.1.2)\n",
      "Requirement already satisfied: click>=8.1.3 in .\\.venv\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in .\\.venv\\lib\\site-packages (from flask) (1.7.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in .\\.venv\\lib\\site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: pycparser in .\\.venv\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in .\\.venv\\lib\\site-packages (4.36.1)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in .\\.venv\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\.venv\\lib\\site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in .\\.venv\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in .\\.venv\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in .\\.venv\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests->transformers) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in .\\.venv\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: Pillow in .\\.venv\\lib\\site-packages (from diffusers) (10.1.0)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from diffusers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in .\\.venv\\lib\\site-packages (from diffusers) (0.19.4)\n",
      "Requirement already satisfied: importlib-metadata in .\\.venv\\lib\\site-packages (from diffusers) (7.0.0)\n",
      "Requirement already satisfied: numpy in .\\.venv\\lib\\site-packages (from diffusers) (1.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\.venv\\lib\\site-packages (from diffusers) (2023.10.3)\n",
      "Requirement already satisfied: requests in .\\.venv\\lib\\site-packages (from diffusers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in .\\.venv\\lib\\site-packages (from diffusers) (0.4.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in .\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.4->diffusers) (2023.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in .\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.4->diffusers) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.4->diffusers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.4->diffusers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in .\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.4->diffusers) (23.2)\n",
      "Requirement already satisfied: zipp>=0.5 in .\\.venv\\lib\\site-packages (from importlib-metadata->diffusers) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests->diffusers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests->diffusers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests->diffusers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests->diffusers) (2023.11.17)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.4->diffusers) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in .\\.venv\\lib\\site-packages (10.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install flask sounddevice numpy\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers\n",
    "!pip install diffusers\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu121/torch_stable.html\n",
      "Requirement already satisfied: torch in .\\.venv\\lib\\site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: torchvision in .\\.venv\\lib\\site-packages (0.16.2+cu121)\n",
      "Requirement already satisfied: torchaudio in .\\.venv\\lib\\site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in .\\.venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in .\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in .\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in .\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in .\\.venv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in .\\.venv\\lib\\site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in .\\.venv\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in .\\.venv\\lib\\site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in .\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu121/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in .\\.venv\\lib\\site-packages (8.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in .\\.venv\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in .\\.venv\\lib\\site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in .\\.venv\\lib\\site-packages (from ipywidgets) (5.14.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in .\\.venv\\lib\\site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in .\\.venv\\lib\\site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: decorator in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in .\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in .\\.venv\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in .\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in .\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in .\\.venv\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in .\\.venv\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8d5e5c117d48f8a26cca0d7936e737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:3009\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [08/Aug/2024 21:10:48] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording done.\n",
      "Rocket Bike\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2d81146a9f4248b599996d6cb55464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [08/Aug/2024 21:25:28] \"POST /process_voice HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Aug/2024 21:25:28] \"GET /static/%20Rocket%20Bike.jpg HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "model_id = \"distil-whisper/distil-small.en\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "voice_to_text_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "voice_to_text_model.to(device)\n",
    "\n",
    "voice_processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "voice_to_text_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=voice_to_text_model,\n",
    "    tokenizer=voice_processor.tokenizer,\n",
    "    feature_extractor=voice_processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/process_voice', methods=['POST'])\n",
    "def process_voice():\n",
    "    if request.method == 'POST':\n",
    "\n",
    "        audio_sample = record_voice(duration=5)\n",
    "\n",
    "\n",
    "        result = voice_to_text_pipe(audio_sample)\n",
    "        v_to_text = result['text']\n",
    "        print(v_to_text.strip())\n",
    "\n",
    "        prompt =v_to_text\n",
    "\n",
    "        images = pipe(prompt=prompt).images[0]\n",
    "        \n",
    "        image_name = f\"{v_to_text}.jpg\"\n",
    "        image_path = f\"static/{image_name}\"\n",
    "        images.save(image_path)\n",
    "        images\n",
    "\n",
    "        return render_template('index.html', v_to_text=v_to_text)\n",
    "\n",
    "\n",
    "def record_voice(duration=5, sample_rate=16000):\n",
    "    print(\"Recording...\")\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype=np.int16)\n",
    "    sd.wait()\n",
    "    print(\"Recording done.\")\n",
    "    # print(audio_data.flatten().astype(np.float32) / np.iinfo(np.int16).max)\n",
    "    return audio_data.flatten().astype(np.float32) / np.iinfo(np.int16).max\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run( port=3009, debug=False,use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7707bb7490e74bf2a6e8ae1ce1349bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:3009\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [08/Aug/2024 08:08:19] \"GET / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording done.\n",
      " Dog have feathers on wings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54c206547ff4d26a386f29263d99d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-08 08:21:01,837] ERROR in app: Exception on /process_voice [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\flask\\app.py\", line 1455, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\flask\\app.py\", line 869, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\flask\\app.py\", line 867, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\flask\\app.py\", line 852, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_27260\\1731831121.py\", line 56, in process_voice\n",
      "    images = pipe(prompt=prompt).images[0]\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\diffusers\\pipelines\\stable_diffusion_xl\\pipeline_stable_diffusion_xl.py\", line 1190, in __call__\n",
      "    image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False)[0]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\diffusers\\utils\\accelerate_utils.py\", line 46, in wrapper\n",
      "    return method(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\diffusers\\models\\autoencoder_kl.py\", line 303, in decode\n",
      "    decoded = self._decode(z).sample\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\diffusers\\models\\autoencoder_kl.py\", line 274, in _decode\n",
      "    dec = self.decoder(z)\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\diffusers\\models\\vae.py\", line 334, in forward\n",
      "    sample = up_block(sample, latent_embeds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\diffusers\\models\\unet_2d_blocks.py\", line 2535, in forward\n",
      "    hidden_states = upsampler(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\diffusers\\models\\resnet.py\", line 204, in forward\n",
      "    hidden_states = self.conv(hidden_states, scale)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\voice to image model\\.venv\\Lib\\site-packages\\diffusers\\models\\lora.py\", line 358, in forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "127.0.0.1 - - [08/Aug/2024 08:21:02] \"POST /process_voice HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "# from flask import Flask, render_template, request\n",
    "# import sounddevice as sd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "# from diffusers import DiffusionPipeline\n",
    "# from PIL import Image\n",
    "# import io\n",
    "\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# model_id = \"distil-whisper/distil-small.en\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# voice_to_text_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "#     model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "# )\n",
    "# voice_to_text_model.to(device)\n",
    "\n",
    "# voice_processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# voice_to_text_pipe = pipeline(\n",
    "#     \"automatic-speech-recognition\",\n",
    "#     model=voice_to_text_model,\n",
    "#     tokenizer=voice_processor.tokenizer,\n",
    "#     feature_extractor=voice_processor.feature_extractor,\n",
    "#     max_new_tokens=128,\n",
    "#     torch_dtype=torch_dtype,\n",
    "#     device=device,\n",
    "# )\n",
    "\n",
    "\n",
    "# pipe = DiffusionPipeline.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    "# )\n",
    "# pipe.to(\"cuda\")\n",
    "\n",
    "# @app.route('/')\n",
    "# def index():\n",
    "#     return render_template('index.html')\n",
    "\n",
    "# @app.route('/process_voice', methods=['POST'])\n",
    "# def process_voice():\n",
    "#     if request.method == 'POST':\n",
    "\n",
    "#         audio_sample = record_voice(duration=5)\n",
    "\n",
    "\n",
    "#         result = voice_to_text_pipe(audio_sample)\n",
    "#         v_to_text = result['text']\n",
    "#         print(v_to_text)\n",
    "\n",
    "#         prompt =v_to_text\n",
    "\n",
    "#         images = pipe(prompt=prompt).images[0]\n",
    "        \n",
    "#         image_name = f\"{v_to_text}.jpg\"\n",
    "#         image_path = f\"static/{image_name}\"\n",
    "#         images.save(image_path)\n",
    "#         images\n",
    "\n",
    "#         return render_template('index.html', v_to_text=v_to_text)\n",
    "\n",
    "\n",
    "# def record_voice(duration=5, sample_rate=16000):\n",
    "#     print(\"Recording...\")\n",
    "#     audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype=np.int16)\n",
    "#     sd.wait()\n",
    "#     print(\"Recording done.\")\n",
    "#     # print(audio_data.flatten().astype(np.float32) / np.iinfo(np.int16).max)\n",
    "#     return audio_data.flatten().astype(np.float32) / np.iinfo(np.int16).max\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run( port=3009, debug=False,use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
